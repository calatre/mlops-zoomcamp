{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9122f01",
   "metadata": {},
   "source": [
    "# Homework 1 - Training a simple model\n",
    "Based on the Notebook shared on the courses material and related to the optional video about training a simple model.\n",
    "In the end I basically cleaned up, reorganized, pointed to new data sources and tried to play around a bit.\n",
    "[Homework URL](https://github.com/DataTalksClub/mlops-zoomcamp/blob/main/cohorts/2024/01-intro/homework.md) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302da2c7",
   "metadata": {},
   "source": [
    "## Part 0 - Technicalities\n",
    "Setting up the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aaaadf9",
   "metadata": {},
   "source": [
    "Lets execute in the console directly with ! some installing. This was needd as conda was not working very well for me and I just made a python .venv \n",
    "\n",
    "_Note: I had a lot of technical troubles, with the jupyter notebook crashing all the time!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bd82d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38c97c0",
   "metadata": {},
   "source": [
    "Libraries needed here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b96024a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy\n",
    "!pip install matplotlib\n",
    "!pip install pandas\n",
    "!pip install seaborn\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6740ed",
   "metadata": {},
   "source": [
    "Pylance includes a parquet file reader for our datasources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09aea3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pylance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e1a610",
   "metadata": {},
   "source": [
    "Getting those libraries.\n",
    "At some point I just commented some of those imports, when not using them for some tests, to speed up execution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41062d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#import pickle\n",
    "#import seaborn as sns\n",
    "#import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "#from sklearn.linear_model import Lasso\n",
    "#from sklearn.linear_model import Ridge\n",
    "\n",
    "from sklearn.metrics import root_mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce4e5b8",
   "metadata": {},
   "source": [
    "## Part 1 - First exploration and playing around"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8fc0dd",
   "metadata": {},
   "source": [
    "Import data for Yellow NYC Taxis Jan 2023 in parquet format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e013caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('./data/yellow_tripdata_2023-01.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc9f646",
   "metadata": {},
   "source": [
    "Some information cells to execute independently (to know how many columns, their distribution, averages, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190466fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e602589",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0324933",
   "metadata": {},
   "source": [
    "Creation of a new column for trip duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8edd512",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['duration'] = df.tpep_dropoff_datetime - df.tpep_pickup_datetime\n",
    "df.duration = df.duration.apply(lambda td: td.total_seconds() / 60)\n",
    "\n",
    "df['duration'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e169f8",
   "metadata": {},
   "source": [
    "df_short to compare with original df and check how much is dropped if we filter for trips between 1 minute and 1 hour (60min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d2bb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_short = df[(df.duration >= 1) & (df.duration <= 60)]\n",
    "\n",
    "df_short['duration'].count()\n",
    "df_short['duration'].describe()\n",
    "df_short['duration'].count()/df['duration'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f9af90",
   "metadata": {},
   "source": [
    "From [homework](https://github.com/DataTalksClub/mlops-zoomcamp/blob/main/cohorts/2024/01-intro/homework.md) \n",
    "\n",
    "_Let's apply one-hot encoding to the pickup and dropoff location IDs. We'll use only these two features for our model._\n",
    "\n",
    "-_Turn the dataframe into a list of dictionaries (remember to re-cast the ids to strings - otherwise it will label encode them)_\n",
    "\n",
    "-_Fit a dictionary vectorizer_\n",
    "\n",
    "-_Get a feature matrix from it_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116ff0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_short #lets now take the short as our main working set\n",
    "\n",
    "df['PU_DO'] = df['PULocationID'] + '_' + df['DOLocationID']\n",
    "categorical = ['PU_DO'] #'PULocationID', 'DOLocationID'] #one hot encoding\n",
    "numerical = ['trip_distance']\n",
    "df[categorical] = df[categorical].astype(str)\n",
    "\n",
    "train_dicts = df[categorical + numerical].to_dict(orient='records')\n",
    "\n",
    "dv = DictVectorizer()\n",
    "X_train = dv.fit_transform(train_dicts)\n",
    "\n",
    "target = 'duration'\n",
    "y_train = df[target].values\n",
    "\n",
    "#We're using a simple linear regression as requested on homework\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "y_pred = lr.predict(X_train)\n",
    "\n",
    "root_mean_squared_error(y_train, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666a8f15",
   "metadata": {},
   "source": [
    "### Optional Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b134c97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(y_pred, label='prediction')\n",
    "sns.histplot(y_train, label='actual')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e417420a",
   "metadata": {},
   "source": [
    "## Part 2 - Main Part\n",
    "\n",
    "Creating the import function and cleaning, so that we can have some easy repeatability of what we've done previously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e6479e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataframe(filename):\n",
    "    df = pd.read_parquet(filename)\n",
    "\n",
    "    df['duration'] = df.tpep_dropoff_datetime - df.tpep_pickup_datetime\n",
    "    df.duration = df.duration.apply(lambda td: td.total_seconds() / 60)\n",
    "\n",
    "    df = df[(df.duration >= 1) & (df.duration <= 60)]\n",
    "\n",
    "    df['PU_DO'] = df['PULocationID'].astype(str) + '_' + df['DOLocationID'].astype(str)\n",
    "    categorical = ['PU_DO'] #'PULocationID', 'DOLocationID']\n",
    "    df[categorical] = df[categorical].astype(str)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d149ca",
   "metadata": {},
   "source": [
    "### Importing Data\n",
    "_Separated into different cells to control and test execution, as the kernel was constantly crashing!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8029eba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = read_dataframe('./data/yellow_tripdata_2023-01.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368b4685",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val = read_dataframe('./data/yellow_tripdata_2023-02.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea0a4a3",
   "metadata": {},
   "source": [
    "Doublecheck sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f2f0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_train), len(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951d51ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['PU_DO'] = df_train['PULocationID'] + '_' + df_train['DOLocationID']  \n",
    "df_val['PU_DO'] = df_val['PULocationID'] + '_' + df_val['DOLocationID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cbfc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "dv = DictVectorizer()\n",
    "\n",
    "categorical = ['PU_DO'] #'PULocationID', 'DOLocationID']\n",
    "df_train[categorical] = df_train[categorical].astype(str)\n",
    "df_val[categorical] = df_val[categorical].astype(str)\n",
    "\n",
    "numerical = ['trip_distance']\n",
    "\n",
    "train_dicts = df_train[categorical + numerical].to_dict(orient='records')\n",
    "X_train = dv.fit_transform(train_dicts)\n",
    "\n",
    "val_dicts = df_val[categorical + numerical].to_dict(orient='records')\n",
    "X_val = dv.transform(val_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9fb68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'duration'\n",
    "y_train = df_train[target].values\n",
    "y_val = df_val[target].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429e2394",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "y_pred = lr.predict(X_train)\n",
    "\n",
    "root_mean_squared_error(y_train, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4882aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_val, y_val)\n",
    "\n",
    "y_pred = lr.predict(X_val)\n",
    "\n",
    "root_mean_squared_error(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cc8e96",
   "metadata": {},
   "source": [
    "Pickle doesn't seem to work well in jupyter, just let it here for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bf6f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('models/lin_reg.bin', 'wb') as f_out:\n",
    "    pickle.dump((dv, lr), f_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fa7793",
   "metadata": {},
   "source": [
    "Other alternatives that were left here for reference (lasso algorithm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4999b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = Lasso(0.01)\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "y_pred = lr.predict(X_val)\n",
    "\n",
    "root_mean_squared_error(y_val, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
